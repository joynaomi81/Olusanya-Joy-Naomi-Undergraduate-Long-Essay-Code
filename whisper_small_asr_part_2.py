# -*- coding: utf-8 -*-
"""Whisper- small ASR Part 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ij-NPtY6FgR2znoSx8d3eDwoEgrXlO5J
"""

!pip install transformers datasets torchaudio accelerate jiwer wandb

import torch
import wandb
import torchaudio
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from torch.optim import AdamW
from jiwer import wer
from tqdm import tqdm
from sklearn.model_selection import train_test_split

wandb.init(
    project="whisper-ASR-yoruba-finetune",
    name="run-1",
    config={
        "model": "openai/whisper-small",
        "language": "yoruba",
        "batch_size": 4,
        "learning_rate": 1e-5,
        "epochs": 3,
    }
)

csv_path = "/content/drive/MyDrive/yoruba_dataset_with_no_tone.csv"
df = pd.read_csv(csv_path)

df.head()

class WhisperDataset(Dataset):
    def __init__(self, df, processor):
        self.data = df.reset_index(drop=True)
        self.processor = processor

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        audio_path = self.data.loc[idx, "full_audio_path"]
        transcription = self.data.loc[idx, "no_tone"]

        waveform, sr = torchaudio.load(audio_path)
        if sr != 16000:
            waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)
        waveform = waveform.squeeze().numpy()

        input_features = self.processor.feature_extractor(
            waveform, sampling_rate=16000
        ).input_features[0]

        labels = self.processor.tokenizer(transcription).input_ids

        return {
            "input_features": torch.tensor(input_features, dtype=torch.float),
            "labels": torch.tensor(labels, dtype=torch.long),
            "text": transcription,
        }

import jiwer

def tone_error_rate(references, predictions):
    transformation = jiwer.Compose([
        jiwer.ToLowerCase(),
        jiwer.RemoveMultipleSpaces(),
        jiwer.Strip()
    ])
    return jiwer.wer(references, predictions, truth_transform=transformation, hypothesis_transform=transformation)

import numpy as np

# Tone mapping for Yoruba vowels
TONE_MAP = {
    'á': ('a', 'H'), 'à': ('a', 'L'), 'a': ('a', 'M'),
    'é': ('e', 'H'), 'è': ('e', 'L'), 'e': ('e', 'M'),
    'ẹ́': ('ẹ', 'H'), 'ẹ̀': ('ẹ', 'L'), 'ẹ': ('ẹ', 'M'),
    'í': ('i', 'H'), 'ì': ('i', 'L'), 'i': ('i', 'M'),
    'ó': ('o', 'H'), 'ò': ('o', 'L'), 'o': ('o', 'M'),
    'ọ́': ('ọ', 'H'), 'ọ̀': ('ọ', 'L'), 'ọ': ('ọ', 'M'),
    'ú': ('u', 'H'), 'ù': ('u', 'L'), 'u': ('u', 'M')
}

def extract_tone_sequence(text):
    tone_sequence = []
    for char in text:
        if char in TONE_MAP:
            _, tone = TONE_MAP[char]
            tone_sequence.append(tone)
    return tone_sequence

def compute_levenshtein(ref, hyp):
    dp = np.zeros((len(ref) + 1, len(hyp) + 1), dtype=int)
    for i in range(len(ref) + 1):
        dp[i][0] = i
    for j in range(len(hyp) + 1):
        dp[0][j] = j
    for i in range(1, len(ref) + 1):
        for j in range(1, len(hyp) + 1):
            cost = 0 if ref[i-1] == hyp[j-1] else 1
            dp[i][j] = min(dp[i-1][j] + 1,
                           dp[i][j-1] + 1,
                           dp[i-1][j-1] + cost)
    return dp[-1][-1]

def tone_error_rate(references, predictions):
    total_tones = 0
    total_errors = 0
    for ref_text, pred_text in zip(references, predictions):
        ref_tones = extract_tone_sequence(ref_text)
        pred_tones = extract_tone_sequence(pred_text)
        total_tones += len(ref_tones)
        total_errors += compute_levenshtein(ref_tones, pred_tones)
    return total_errors / total_tones if total_tones > 0 else 0.0

def evaluate(model, dataloader, processor, device):
    model.eval()
    val_loss = 0
    preds = []
    refs = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            inputs = batch["input_features"].to(device)
            labels = batch["labels"].to(device)

            outputs = model(input_features=inputs, labels=labels)
            val_loss += outputs.loss.item()

            predicted_ids = model.generate(inputs)
            transcripts = processor.batch_decode(predicted_ids, skip_special_tokens=True)

            preds.extend(transcripts)
            refs.extend(batch["text"])

    wer_score = wer(refs, preds)
    ter_score = tone_error_rate(refs, preds)

    return val_loss / len(dataloader), wer_score, ter_score

csv_path = "/content/drive/MyDrive/yoruba_dataset_with_no_tone.csv"
df = pd.read_csv(csv_path)

train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)

from transformers import WhisperTokenizer, WhisperProcessor

model_name = "openai/whisper-small"
processor = WhisperProcessor.from_pretrained(model_name)
model = WhisperForConditionalGeneration.from_pretrained(model_name)

# Get language and task token IDs
tokenizer: WhisperTokenizer = processor.tokenizer
forced_decoder_ids = tokenizer.get_decoder_prompt_ids(
    language="yoruba", task="transcribe"
)

# Set the model config
model.config.forced_decoder_ids = forced_decoder_ids

def collate_fn(batch):
    input_features = [item["input_features"] for item in batch]
    label_features = [item["labels"] for item in batch]
    texts = [item["text"] for item in batch]  # Keep reference text for evaluation

    # Pad input features and labels
    input_features = processor.feature_extractor.pad(
        {"input_features": input_features},
        return_tensors="pt"
    )["input_features"]

    label_features = processor.tokenizer.pad(
        {"input_ids": label_features},
        return_tensors="pt"
    )["input_ids"]

    return {
        "input_features": input_features,
        "labels": label_features,
        "text": texts
    }

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)

import torch
torch.cuda.empty_cache()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
optimizer = AdamW(model.parameters(), lr=1e-5)

for epoch in range(3):
    model.train()
    total_loss = 0

    for batch in tqdm(train_loader, desc=f"Epoch {epoch+1} Training"):
        inputs = batch["input_features"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(input_features=inputs, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        total_loss += loss.item()

        wandb.log({"train/loss": loss.item()})

    avg_train_loss = total_loss / len(train_loader)

    #  Get TER from evaluation
    val_loss, val_wer, val_ter = evaluate(model, val_loader, processor, device)

    print(f"\nEpoch {epoch+1}:")
    print(f"  Train Loss: {avg_train_loss:.4f}")
    print(f"  Val Loss:   {val_loss:.4f}")
    print(f"  Val WER:    {val_wer:.4f}")
    print(f"  Val TER:    {val_ter:.4f}\n")  #Print TER

    #  Log TER to Weights & Biases
    wandb.log({
        "epoch": epoch + 1,
        "train/avg_loss": avg_train_loss,
        "val/loss": val_loss,
        "val/wer": val_wer,
        "val/ter": val_ter  #Log TER
    })

wandb.finish()