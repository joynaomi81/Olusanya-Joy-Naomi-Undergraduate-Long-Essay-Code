# -*- coding: utf-8 -*-
"""Word-Level ASR Tranining

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gP68_r1kv84neByh9AFCz0bKV8m4psgx
"""

import zipfile
import os
import shutil

# Define paths
zip_path = '/content/drive/MyDrive/Colab Notebooks/Lexical_words (1).zip'
extract_dir = '/content/lexical_words_extracted'
audio_target_dir = '/content/first_100_audio'

# Unzip folder
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

#recorder.tsv
tsv_path = None
for root, dirs, files in os.walk(extract_dir):
    for file in files:
        if file == 'recorder.tsv':
            tsv_path = os.path.join(root, file)
            break

# Copy recorder.tsv to /content/
if tsv_path:
    shutil.copy(tsv_path, '/content/recorder.tsv')
    print(" recorder.tsv copied to /content/")
else:
    print("recorder.tsv not found.")

# Laod first 100 audio files to /content/first_100_audio
os.makedirs(audio_target_dir, exist_ok=True)
audio_files = []

for root, dirs, files in os.walk(extract_dir):
    for file in sorted(files):
        if file.lower().endswith(('.wav', '.mp3', '.flac')):
            audio_files.append(os.path.join(root, file))

# Copy the first 100 audio files
for audio_file in audio_files[:100]:
    shutil.copy(audio_file, audio_target_dir)

print(f" Copied {min(100, len(audio_files))} audio files to {audio_target_dir}")

import pandas as pd

# Load the TSV file (no header in the file)
df = pd.read_csv('/content/recorder.tsv', sep='\t', header=None)

# Assign column names manually
df.columns = ['file_path', 'speaker_id', 'category', 'misc', 'lexical_word']

# Show first 5 rows
df.head()

import os
import pandas as pd

#Load the 100 audio filenames
audio_dir = '/content/first_100_audio'
audio_files = sorted(os.listdir(audio_dir))

#Load the recorder.tsv without headers
df = pd.read_csv('/content/recorder.tsv', sep='\t', header=None)

#Assign column names manually
df.columns = ['file_path', 'speaker_id', 'category', 'misc', 'lexical_word']

#Extract just the filename from the file_path
df['filename'] = df['file_path'].apply(lambda x: os.path.basename(x))

#Filter rows that match the 100 audio files
df_filtered = df[df['filename'].isin(audio_files)].reset_index(drop=True)

#Add actual audio file path
df_filtered['audio_path'] = df_filtered['filename'].apply(lambda x: os.path.join(audio_dir, x))

# Print the first few matched rows
df_filtered[['filename', 'lexical_word', 'audio_path']].head()

#Create new DataFrame with only selected columns
df_fixed = df_filtered[['filename', 'lexical_word', 'audio_path']].copy()

#print the result
df_fixed.head()

from IPython.display import Audio

row = df_filtered.iloc[0]
print(f"Lexical word: {row['lexical_word']}")
Audio(row['audio_path'])

import torchaudio
import matplotlib.pyplot as plt
from IPython.display import Audio

row = df_filtered.iloc[0]

# Load the audio
waveform, sample_rate = torchaudio.load(row['audio_path'])

# Plot the spectrogram
spectrogram = torchaudio.transforms.Spectrogram()(waveform)

plt.figure(figsize=(10, 4))
plt.imshow(spectrogram.log2()[0, :, :].numpy(), aspect='auto', origin='lower')
plt.title('Spectrogram')
plt.xlabel('Time (frames)')
plt.ylabel('Frequency bins')
plt.colorbar(format='%+2.0f dB')
plt.tight_layout()
plt.show()

df_fixed.shape

!pip install -q git+https://github.com/openai/whisper.git
!sudo apt update && sudo apt install -y ffmpeg

from tqdm import tqdm

predictions = []

# Loop with progress bar
for path in tqdm(df_fixed['audio_path'], desc="Transcribing audio"):
    result = model.transcribe(path, language='yo')
    predictions.append(result['text'].strip())

"""With AstralZander/yoruba_ASR"""

import torch
import torchaudio
from transformers import AutoProcessor, AutoModelForCTC
from tqdm import tqdm

# Load Yoruba ASR model and processor
processor = AutoProcessor.from_pretrained("AstralZander/yoruba_ASR")
model = AutoModelForCTC.from_pretrained("AstralZander/yoruba_ASR")

# Ensure model is on the right device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

def transcribe_audio(path):
    # Load and resample the audio
    waveform, sample_rate = torchaudio.load(path)
    if sample_rate != 16_000:
        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16_000)
        waveform = resampler(waveform)

    # Preprocess
    inputs = processor(waveform.squeeze(), sampling_rate=16_000, return_tensors="pt", padding=True)
    input_values = inputs.input_values.to(device)

    # Predict
    with torch.no_grad():
        logits = model(input_values).logits

    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = processor.batch_decode(predicted_ids)[0].strip()

    return transcription

# Run transcription on your audio files
predictions = []
for path in tqdm(df_fixed['audio_path'], desc="Transcribing with Yoruba ASR"):
    pred = transcribe_audio(path)
    predictions.append(pred)

# Add to DataFrame
df_fixed['yoruba_asr_prediction'] = predictions

from rapidfuzz import fuzz

def show_transcription_evaluation(df, index):
    row = df.iloc[index]
    print(f" Audio: {row['audio_path']}")
    print(f" Ground Truth:      {row['lexical_word']}")
    print(f" Yoruba ASR Output: {row['yoruba_asr_prediction']}")

    score = fuzz.ratio(row['lexical_word'].lower().strip(), row['yoruba_asr_prediction'].lower().strip())
    print(f"Similarity Score:   {score:.2f} / 100")
    print("Match" if score == 100 else "Mismatch")

# Example: Show for first 5 rows
for i in range(20):
    show_transcription_evaluation(df_fixed, i)

!pip install -q transformers torchaudio librosa

"""With facebook/mms-1b-all"""

from transformers import AutoProcessor, AutoModelForCTC
import torch
import torchaudio
from tqdm import tqdm

# Load Facebook MMS multilingual ASR model
processor = AutoProcessor.from_pretrained("facebook/mms-1b-all")
model = AutoModelForCTC.from_pretrained("facebook/mms-1b-all")

# Use GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

def transcribe_audio_mms(audio_path):
    waveform, sample_rate = torchaudio.load(audio_path)

    # (MMS expects 16kHz)
    if sample_rate != 16000:
        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)
        waveform = resampler(waveform)

    inputs = processor(waveform.squeeze(), sampling_rate=16000, return_tensors="pt")
    input_values = inputs.input_values.to(device)

    with torch.no_grad():
        logits = model(input_values).logits

    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = processor.batch_decode(predicted_ids)[0].strip()

    return transcription

# Loop through the dataset and get predictions
mms_predictions = []
for path in tqdm(df_fixed['audio_path'], desc="Transcribing with MMS"):
    pred = transcribe_audio_mms(path)
    mms_predictions.append(pred)

# Add to DataFrame
df_fixed['mms_prediction'] = mms_predictions

from rapidfuzz import fuzz

def show_mms_eval(df, index):
    row = df.iloc[index]
    gt = row['lexical_word']
    pred = row['mms_prediction']

    score = fuzz.ratio(gt.lower().strip(), pred.lower().strip())

    print(f"\nAudio: {row['audio_path']}")
    print(f" Ground Truth:      {gt}")
    print(f"MMS Prediction:     {pred}")
    print(f"Similarity Score:   {score:.2f} / 100")
    print("Match" if score == 100 else " Mismatch")

# Show first 5
for i in range(20):
    show_mms_eval(df_fixed, i)

!pip install -q rapidfuzz