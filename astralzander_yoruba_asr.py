# -*- coding: utf-8 -*-
"""AstralZander/yoruba ASR

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13VUrrvNA0BaVfo0W4k5z8F0-qfPQ7IZ4
"""

!pip install transformers datasets torchaudio accelerate jiwer wandb

import torch
import wandb
import torchaudio
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from torch.optim import AdamW
from jiwer import wer
from tqdm import tqdm
from sklearn.model_selection import train_test_split

"""TONE ASR Training"""

# %%
import torch
import wandb
import torchaudio
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from transformers import AutoProcessor, AutoModelForCTC
import jiwer
import numpy as np

# %%
# Initialize wandb
wandb.init(
    project="XLS-ASR-tone-yoruba-finetune",
    name="run-1",
    config={
        "model": "facebook/wav2vec2-xls-r-300m",
        "language": "yoruba",
        "batch_size": 4,
        "learning_rate": 1e-5,
        "epochs": 3,
    }
)

# %%
class AudioDataset(Dataset):
    def __init__(self, df, processor):
        self.data = df.reset_index(drop=True)
        self.processor = processor

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        audio_path = self.data.loc[idx, "full_audio_path"]
        transcription = self.data.loc[idx, "sentence"]

        try:
            waveform, sr = torchaudio.load(audio_path)
            if sr != 16000:
                waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)
            waveform = waveform.squeeze().numpy()
            if waveform.size == 0:
                print(f"Skipping empty audio file at {audio_path}")
                return None

            input_features = self.processor(
                waveform, sampling_rate=16000, return_tensors="pt"
            ).input_values[0]

            if not transcription or pd.isna(transcription):
                print(f"Skipping item with empty transcription for {audio_path}")
                return None

            labels = self.processor.tokenizer(transcription, return_attention_mask=False).input_ids
            if not labels or not isinstance(labels, list):
                print(f"Invalid labels for transcription '{transcription}' at {audio_path}")
                return None

            return {
                "input_features": input_features,
                "labels": torch.tensor(labels, dtype=torch.long),
                "text": transcription,
            }
        except Exception as e:
            print(f"Error processing item {idx} at {audio_path}: {e}")
            return None

# %%
# Tone mapping for Yoruba vowels
TONE_MAP = {
    'á': ('a', 'H'), 'à': ('a', 'L'), 'a': ('a', 'M'),
    'é': ('e', 'H'), 'è': ('e', 'L'), 'e': ('e', 'M'),
    'ẹ́': ('ẹ', 'H'), 'ẹ̀': ('ẹ', 'L'), 'ẹ': ('ẹ', 'M'),
    'í': ('i', 'H'), 'ì': ('i', 'L'), 'i': ('i', 'M'),
    'ó': ('o', 'H'), 'ò': ('o', 'L'), 'o': ('o', 'M'),
    'ọ́': ('ọ', 'H'), 'ọ̀': ('ọ', 'L'), 'ọ': ('ọ', 'M'),
    'ú': ('u', 'H'), 'ù': ('u', 'L'), 'u': ('u', 'M')
}

def extract_tone_sequence(text):
    tone_sequence = []
    for char in text:
        if char in TONE_MAP:
            _, tone = TONE_MAP[char]
            tone_sequence.append(tone)
    return tone_sequence

def compute_levenshtein(ref, hyp):
    dp = np.zeros((len(ref) + 1, len(hyp) + 1), dtype=int)
    for i in range(len(ref) + 1):
        dp[i][0] = i
    for j in range(len(hyp) + 1):
        dp[0][j] = j
    for i in range(1, len(ref) + 1):
        for j in range(1, len(hyp) + 1):
            cost = 0 if ref[i-1] == hyp[j-1] else 1
            dp[i][j] = min(
                dp[i-1][j] + 1,
                dp[i][j-1] + 1,
                dp[i-1][j-1] + cost
            )
    return dp[-1][-1]

def tone_error_rate(references, predictions):
    total_tones = 0
    total_errors = 0
    for ref_text, pred_text in zip(references, predictions):
        ref_tones = extract_tone_sequence(ref_text)
        pred_tones = extract_tone_sequence(pred_text)
        total_tones += len(ref_tones)
        total_errors += compute_levenshtein(ref_tones, pred_tones)
    return total_errors / total_tones if total_tones > 0 else 0.0

# %%
def evaluate(model, dataloader, processor, device):
    model.eval()
    preds = []
    refs = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            if batch is None:
                continue

            inputs = batch["input_features"].to(device)
            labels = batch["labels"].to(device)

            logits = model(inputs).logits
            predicted_ids = torch.argmax(logits, dim=-1)
            transcripts = processor.batch_decode(predicted_ids, skip_special_tokens=True)

            preds.extend(transcripts)
            refs.extend(batch["text"])

    wer_score = jiwer.wer(refs, preds)
    ter_score = tone_error_rate(refs, preds)
    return wer_score, ter_score

# %%
# Load data
csv_path = "/content/drive/MyDrive/yoruba_dataset_with_no_tone.csv"
df = pd.read_csv(csv_path)
train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)

# %%
# Load model and processor

from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC

processor = AutoProcessor.from_pretrained("AstralZander/yoruba_ASR")
model = AutoModelForCTC.from_pretrained("AstralZander/yoruba_ASR")
def collate_fn(batch):
    batch = [item for item in batch if item is not None]
    if not batch:
        return None

    input_features = [item["input_features"] for item in batch]
    label_features = [item["labels"] for item in batch]
    texts = [item["text"] for item in batch]

    input_features = torch.nn.utils.rnn.pad_sequence(input_features, batch_first=True)

    padded_labels = processor.tokenizer.pad(
        {"input_ids": label_features},
        return_tensors="pt",
        padding="longest"
    )
    label_features = padded_labels["input_ids"]

    return {
        "input_features": input_features,
        "labels": label_features,
        "text": texts,
    }

train_dataset = AudioDataset(train_df, processor)
val_dataset = AudioDataset(val_df, processor)

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
optimizer = AdamW(model.parameters(), lr=1e-5)

for epoch in range(wandb.config.epochs):
    model.train()
    total_loss = 0

    for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1} Training"):
        if batch is None:
            print("Skipping empty batch")
            continue

        inputs = batch["input_features"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(input_values=inputs, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        total_loss += loss.item()
        wandb.log({"train/loss": loss.item()})

    avg_train_loss = total_loss / len(train_loader)
    val_wer, val_ter = evaluate(model, val_loader, processor, device)

    wandb.log({
        "epoch": epoch + 1,
        "train/loss_avg": avg_train_loss,
        "val/wer": val_wer,
        "val/ter": val_ter,
    })

    print(f"Epoch {epoch + 1} | Train Loss: {avg_train_loss:.4f} | Val WER: {val_wer:.4f} | Val TER: {val_ter:.4f}")

import torch
import torchaudio
from transformers import AutoProcessor, AutoModelForCTC

# Device setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize wandb
wandb.init(
    project="XLS-ASR-no-tone-yoruba-finetune",
    name="run-1",  # change per experiment
    config={
        "model": "AstralZander/yoruba_ASR",
        "language": "yoruba",
        "batch_size": 4,
        "learning_rate": 1e-5,
        "epochs": 3,
    }
)


class AudioDataset(Dataset):
    def __init__(self, df, processor):
        self.data = df.reset_index(drop=True)
        self.processor = processor

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        audio_path = self.data.loc[idx, "full_audio_path"]
        transcription = self.data.loc[idx, "no_tone"]

        try:
            waveform, sr = torchaudio.load(audio_path)
            if sr != 16000:
                waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)
            waveform = waveform.squeeze().numpy()
            if waveform.size == 0:
                print(f"Skipping empty audio file at {audio_path}")
                return None

            input_features = self.processor(
                waveform, sampling_rate=16000, return_tensors="pt"
            ).input_values[0]

            if not transcription or pd.isna(transcription):
                print(f"Skipping item with empty transcription for {audio_path}")
                return None

            labels = self.processor.tokenizer(transcription, return_attention_mask=False).input_ids
            if not labels or not isinstance(labels, list):
                print(f"Invalid labels for transcription '{transcription}' at {audio_path}")
                return None

            return {
                "input_features": input_features,
                "labels": torch.tensor(labels, dtype=torch.long),
                "text": transcription,
            }
        except Exception as e:
            print(f"Error processing item {idx} at {audio_path}: {e}")
            return None


# Tone mapping for Yoruba vowels
TONE_MAP = {
    'á': ('a', 'H'), 'à': ('a', 'L'), 'a': ('a', 'M'),
    'é': ('e', 'H'), 'è': ('e', 'L'), 'e': ('e', 'M'),
    'ẹ́': ('ẹ', 'H'), 'ẹ̀': ('ẹ', 'L'), 'ẹ': ('ẹ', 'M'),
    'í': ('i', 'H'), 'ì': ('i', 'L'), 'i': ('i', 'M'),
    'ó': ('o', 'H'), 'ò': ('o', 'L'), 'o': ('o', 'M'),
    'ọ́': ('ọ', 'H'), 'ọ̀': ('ọ', 'L'), 'ọ': ('ọ', 'M'),
    'ú': ('u', 'H'), 'ù': ('u', 'L'), 'u': ('u', 'M')
}

def extract_tone_sequence(text):
    tone_sequence = []
    for char in text:
        if char in TONE_MAP:
            _, tone = TONE_MAP[char]
            tone_sequence.append(tone)
    return tone_sequence

def compute_levenshtein(ref, hyp):
    dp = np.zeros((len(ref) + 1, len(hyp) + 1), dtype=int)
    for i in range(len(ref) + 1):
        dp[i][0] = i
    for j in range(len(hyp) + 1):
        dp[0][j] = j
    for i in range(1, len(ref) + 1):
        for j in range(1, len(hyp) + 1):
            cost = 0 if ref[i-1] == hyp[j-1] else 1
            dp[i][j] = min(
                dp[i-1][j] + 1,
                dp[i][j-1] + 1,
                dp[i-1][j-1] + cost
            )
    return dp[-1][-1]

def tone_error_rate(references, predictions):
    total_tones = 0
    total_errors = 0
    for ref_text, pred_text in zip(references, predictions):
        ref_tones = extract_tone_sequence(ref_text)
        pred_tones = extract_tone_sequence(pred_text)
        total_tones += len(ref_tones)
        total_errors += compute_levenshtein(ref_tones, pred_tones)
    return total_errors / total_tones if total_tones > 0 else 0.0

# %%
def evaluate(model, dataloader, processor, device):
    model.eval()
    preds = []
    refs = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            if batch is None:
                continue

            inputs = batch["input_features"].to(device)
            labels = batch["labels"].to(device)

            logits = model(inputs).logits
            predicted_ids = torch.argmax(logits, dim=-1)
            transcripts = processor.batch_decode(predicted_ids, skip_special_tokens=True)

            preds.extend(transcripts)
            refs.extend(batch["text"])

    wer_score = jiwer.wer(refs, preds)
    ter_score = tone_error_rate(refs, preds)
    return wer_score, ter_score

# %%
# Load data
csv_path = "/content/drive/MyDrive/yoruba_dataset_with_no_tone.csv"
df = pd.read_csv(csv_path)
train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)

# %%
# Load model and processor
processor = AutoProcessor.from_pretrained("AstralZander/yoruba_ASR")
model = AutoModelForCTC.from_pretrained("AstralZander/yoruba_ASR")

# %%
def collate_fn(batch):
    batch = [item for item in batch if item is not None]
    if not batch:
        return None

    input_features = [item["input_features"] for item in batch]
    label_features = [item["labels"] for item in batch]
    texts = [item["text"] for item in batch]

    input_features = torch.nn.utils.rnn.pad_sequence(input_features, batch_first=True)

    padded_labels = processor.tokenizer.pad(
        {"input_ids": label_features},
        return_tensors="pt",
        padding="longest"
    )
    label_features = padded_labels["input_ids"]

    return {
        "input_features": input_features,
        "labels": label_features,
        "text": texts,
    }


train_dataset = AudioDataset(train_df, processor)
val_dataset = AudioDataset(val_df, processor)

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
optimizer = AdamW(model.parameters(), lr=1e-5)


for epoch in range(wandb.config.epochs):
    model.train()
    total_loss = 0

    for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1} Training"):
        if batch is None:
            print("Skipping empty batch")
            continue

        inputs = batch["input_features"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(input_values=inputs, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        total_loss += loss.item()
        wandb.log({"train/loss": loss.item()})

    avg_train_loss = total_loss / len(train_loader)
    val_wer, val_ter = evaluate(model, val_loader, processor, device)

    wandb.log({
        "epoch": epoch + 1,
        "train/loss_avg": avg_train_loss,
        "val/wer": val_wer,
        "val/ter": val_ter,
    })

    print(f"Epoch {epoch + 1} | Train Loss: {avg_train_loss:.4f} | Val WER: {val_wer:.4f} | Val TER: {val_ter:.4f}")


def compute_accuracy(predictions, references):
    correct = sum(p.strip() == r.strip() for p, r in zip(predictions, references))
    total = len(references)
    accuracy = correct / total if total > 0 else 0.0
    print(f"Accuracy: {accuracy:.4f}")
    return accuracy

# Evaluate after training
model.eval()
final_preds = []
final_refs = []

with torch.no_grad():
    for batch in tqdm(val_loader, desc="Final Accuracy Evaluation"):
        if batch is None:
            continue

        inputs = batch["input_features"].to(device)
        labels = batch["labels"].to(device)

        logits = model(inputs).logits
        predicted_ids = torch.argmax(logits, dim=-1)
        transcripts = processor.batch_decode(predicted_ids, skip_special_tokens=True)

        final_preds.extend(transcripts)
        final_refs.extend(batch["text"])

# Compute accuracy
compute_accuracy(final_preds, final_refs)

"""Prediction"""

def transcribe_audio(file_path, model, processor, device):
    # Load and preprocess audio
    waveform, sr = torchaudio.load(file_path)
    if sr != 16000:
        waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)
    waveform = waveform.squeeze().numpy()

    # Tokenize input
    input_features = processor(waveform, sampling_rate=16000, return_tensors="pt").input_values.to(device)

    # Run inference
    with torch.no_grad():
        logits = model(input_features).logits
        predicted_ids = torch.argmax(logits, dim=-1)
        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]

    return transcription

reference = "faleti ti lo ra awon sokoto ni oja ejigbo"
real_audio_path = "/content/yof_00295_00020329077.wav"

# Get ASR output
predicted_text = transcribe_audio(real_audio_path, model, processor, device)

# Print both
print("Reference:", reference)
print("Predicted:", predicted_text)

# Optionally compute WER and Tone Error Rate
wer = jiwer.wer(reference, predicted_text)
ter = tone_error_rate([reference], [predicted_text])

print(f"WER: {wer:.4f}")
print(f"Tone Error Rate: {ter:.4f}")