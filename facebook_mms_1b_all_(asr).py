# -*- coding: utf-8 -*-
"""facebook/mms-1b-all (ASR)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14fJ2n5NLgZKLplzbmUcFQKObc-Z0d5_3
"""

import pandas as pd
df = pd.read_csv("/content/drive/MyDrive/yoruba_dataset_with_no_tone.csv")
df.head()

df.shape

!pip install transformers datasets torchaudio accelerate jiwer wandb

"""## Fine-Tuning ASR Model with Tone-Marked Yoruba Dataset"""

#Import the necessary libraries
import torch
import wandb
import torchaudio
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from transformers import AutoProcessor, AutoModelForCTC
import jiwer
import numpy as np


# Initialize wandb
wandb.init(
    project="MMS-ASR-tone-yoruba-finetune",
    name="run-1",
    config={
        "model": "facebook/mms-1b-all",
        "language": "yoruba",
        "batch_size": 4,
        "learning_rate": 1e-5,
        "epochs": 3,
    }
)


class AudioDataset(Dataset):
    def __init__(self, df, processor):
        self.data = df.reset_index(drop=True)
        self.processor = processor

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        audio_path = self.data.loc[idx, "full_audio_path"]
        transcription = self.data.loc[idx, "sentence"]

        try:
            waveform, sr = torchaudio.load(audio_path)
            if sr != 16000:
                waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)
            waveform = waveform.squeeze().numpy()
            if waveform.size == 0:
                print(f"Skipping empty audio file at {audio_path}")
                return None

            input_features = self.processor(
                waveform, sampling_rate=16000, return_tensors="pt"
            ).input_values[0]

            if not transcription or pd.isna(transcription):
                print(f"Skipping item with empty transcription for {audio_path}")
                return None

            labels = self.processor.tokenizer(transcription, return_attention_mask=False).input_ids
            if not labels or not isinstance(labels, list):
                print(f"Invalid labels for transcription '{transcription}' at {audio_path}")
                return None

            return {
                "input_features": input_features,
                "labels": torch.tensor(labels, dtype=torch.long),
                "text": transcription,
            }
        except Exception as e:
            print(f"Error processing item {idx} at {audio_path}: {e}")
            return None


# Tone mapping for Yoruba vowels
TONE_MAP = {
    'á': ('a', 'H'), 'à': ('a', 'L'), 'a': ('a', 'M'),
    'é': ('e', 'H'), 'è': ('e', 'L'), 'e': ('e', 'M'),
    'ẹ́': ('ẹ', 'H'), 'ẹ̀': ('ẹ', 'L'), 'ẹ': ('ẹ', 'M'),
    'í': ('i', 'H'), 'ì': ('i', 'L'), 'i': ('i', 'M'),
    'ó': ('o', 'H'), 'ò': ('o', 'L'), 'o': ('o', 'M'),
    'ọ́': ('ọ', 'H'), 'ọ̀': ('ọ', 'L'), 'ọ': ('ọ', 'M'),
    'ú': ('u', 'H'), 'ù': ('u', 'L'), 'u': ('u', 'M')
}

def extract_tone_sequence(text):
    tone_sequence = []
    for char in text:
        if char in TONE_MAP:
            _, tone = TONE_MAP[char]
            tone_sequence.append(tone)
    return tone_sequence

def compute_levenshtein(ref, hyp):
    dp = np.zeros((len(ref) + 1, len(hyp) + 1), dtype=int)
    for i in range(len(ref) + 1):
        dp[i][0] = i
    for j in range(len(hyp) + 1):
        dp[0][j] = j
    for i in range(1, len(ref) + 1):
        for j in range(1, len(hyp) + 1):
            cost = 0 if ref[i-1] == hyp[j-1] else 1
            dp[i][j] = min(
                dp[i-1][j] + 1,
                dp[i][j-1] + 1,
                dp[i-1][j-1] + cost
            )
    return dp[-1][-1]

def tone_error_rate(references, predictions):
    total_tones = 0
    total_errors = 0
    for ref_text, pred_text in zip(references, predictions):
        ref_tones = extract_tone_sequence(ref_text)
        pred_tones = extract_tone_sequence(pred_text)
        total_tones += len(ref_tones)
        total_errors += compute_levenshtein(ref_tones, pred_tones)
    return total_errors / total_tones if total_tones > 0 else 0.0


def evaluate(model, dataloader, processor, device):
    model.eval()
    preds = []
    refs = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            if batch is None:
                continue

            inputs = batch["input_features"].to(device)
            labels = batch["labels"].to(device)

            logits = model(inputs).logits
            predicted_ids = torch.argmax(logits, dim=-1)
            transcripts = processor.batch_decode(predicted_ids, skip_special_tokens=True)

            preds.extend(transcripts)
            refs.extend(batch["text"])

    wer_score = jiwer.wer(refs, preds)
    ter_score = tone_error_rate(refs, preds)
    return wer_score, ter_score


# Load data
csv_path = "/content/drive/MyDrive/yoruba_dataset_with_no_tone.csv"
df = pd.read_csv(csv_path)
train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)


# Load model and processor
# Load model directly
from transformers import AutoProcessor, AutoModelForCTC

processor = AutoProcessor.from_pretrained("facebook/mms-1b-all")
model = AutoModelForCTC.from_pretrained("facebook/mms-1b-all")

def collate_fn(batch):
    batch = [item for item in batch if item is not None]
    if not batch:
        return None

    input_features = [item["input_features"] for item in batch]
    label_features = [item["labels"] for item in batch]
    texts = [item["text"] for item in batch]

    input_features = torch.nn.utils.rnn.pad_sequence(input_features, batch_first=True)

    padded_labels = processor.tokenizer.pad(
        {"input_ids": label_features},
        return_tensors="pt",
        padding="longest"
    )
    label_features = padded_labels["input_ids"]

    return {
        "input_features": input_features,
        "labels": label_features,
        "text": texts,
    }


train_dataset = AudioDataset(train_df, processor)
val_dataset = AudioDataset(val_df, processor)

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
optimizer = AdamW(model.parameters(), lr=1e-5)


for epoch in range(wandb.config.epochs):
    model.train()
    total_loss = 0

    for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1} Training"):
        if batch is None:
            print("Skipping empty batch")
            continue

        inputs = batch["input_features"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(input_values=inputs, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        total_loss += loss.item()
        wandb.log({"train/loss": loss.item()})

    avg_train_loss = total_loss / len(train_loader)
    val_wer, val_ter = evaluate(model, val_loader, processor, device)

    wandb.log({
        "epoch": epoch + 1,
        "train/loss_avg": avg_train_loss,
        "val/wer": val_wer,
        "val/ter": val_ter,
    })

    print(f"Epoch {epoch + 1} | Train Loss: {avg_train_loss:.4f} | Val WER: {val_wer:.4f} | Val TER: {val_ter:.4f}")

"""## Fine-Tuning ASR Model without Tone-Marked Yoruba Dataset"""

#Import necessary libraries
import torch
import wandb
import torchaudio
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from transformers import AutoProcessor, AutoModelForCTC
import jiwer
import numpy as np

# Initialize wandb
wandb.init(
    project="MMS-ASR-tone-yoruba-finetune",
    name="run-1",
    config={
        "model": "facebook/mms-1b-all",
        "language": "yoruba",
        "batch_size": 4,
        "learning_rate": 1e-5,
        "epochs": 3,
    }
)


class AudioDataset(Dataset):
    def __init__(self, df, processor):
        self.data = df.reset_index(drop=True)
        self.processor = processor

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        audio_path = self.data.loc[idx, "full_audio_path"]
        transcription = self.data.loc[idx, "no_tone"]

        try:
            waveform, sr = torchaudio.load(audio_path)
            if sr != 16000:
                waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)
            waveform = waveform.squeeze().numpy()
            if waveform.size == 0:
                print(f"Skipping empty audio file at {audio_path}")
                return None

            input_features = self.processor(
                waveform, sampling_rate=16000, return_tensors="pt"
            ).input_values[0]

            if not transcription or pd.isna(transcription):
                print(f"Skipping item with empty transcription for {audio_path}")
                return None

            labels = self.processor.tokenizer(transcription, return_attention_mask=False).input_ids
            if not labels or not isinstance(labels, list):
                print(f"Invalid labels for transcription '{transcription}' at {audio_path}")
                return None

            return {
                "input_features": input_features,
                "labels": torch.tensor(labels, dtype=torch.long),
                "text": transcription,
            }
        except Exception as e:
            print(f"Error processing item {idx} at {audio_path}: {e}")
            return None


# Tone mapping for Yoruba vowels
TONE_MAP = {
    'á': ('a', 'H'), 'à': ('a', 'L'), 'a': ('a', 'M'),
    'é': ('e', 'H'), 'è': ('e', 'L'), 'e': ('e', 'M'),
    'ẹ́': ('ẹ', 'H'), 'ẹ̀': ('ẹ', 'L'), 'ẹ': ('ẹ', 'M'),
    'í': ('i', 'H'), 'ì': ('i', 'L'), 'i': ('i', 'M'),
    'ó': ('o', 'H'), 'ò': ('o', 'L'), 'o': ('o', 'M'),
    'ọ́': ('ọ', 'H'), 'ọ̀': ('ọ', 'L'), 'ọ': ('ọ', 'M'),
    'ú': ('u', 'H'), 'ù': ('u', 'L'), 'u': ('u', 'M')
}

def extract_tone_sequence(text):
    tone_sequence = []
    for char in text:
        if char in TONE_MAP:
            _, tone = TONE_MAP[char]
            tone_sequence.append(tone)
    return tone_sequence

def compute_levenshtein(ref, hyp):
    dp = np.zeros((len(ref) + 1, len(hyp) + 1), dtype=int)
    for i in range(len(ref) + 1):
        dp[i][0] = i
    for j in range(len(hyp) + 1):
        dp[0][j] = j
    for i in range(1, len(ref) + 1):
        for j in range(1, len(hyp) + 1):
            cost = 0 if ref[i-1] == hyp[j-1] else 1
            dp[i][j] = min(
                dp[i-1][j] + 1,
                dp[i][j-1] + 1,
                dp[i-1][j-1] + cost
            )
    return dp[-1][-1]

def tone_error_rate(references, predictions):
    total_tones = 0
    total_errors = 0
    for ref_text, pred_text in zip(references, predictions):
        ref_tones = extract_tone_sequence(ref_text)
        pred_tones = extract_tone_sequence(pred_text)
        total_tones += len(ref_tones)
        total_errors += compute_levenshtein(ref_tones, pred_tones)
    return total_errors / total_tones if total_tones > 0 else 0.0


def evaluate(model, dataloader, processor, device):
    model.eval()
    preds = []
    refs = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            if batch is None:
                continue

            inputs = batch["input_features"].to(device)
            labels = batch["labels"].to(device)

            logits = model(inputs).logits
            predicted_ids = torch.argmax(logits, dim=-1)
            transcripts = processor.batch_decode(predicted_ids, skip_special_tokens=True)

            preds.extend(transcripts)
            refs.extend(batch["text"])

    wer_score = jiwer.wer(refs, preds)
    ter_score = tone_error_rate(refs, preds)
    return wer_score, ter_score


# Load data
csv_path = "/content/drive/MyDrive/yoruba_dataset_with_no_tone.csv"
df = pd.read_csv(csv_path)
train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)


# Load model and processor
# Load model directly
from transformers import AutoProcessor, AutoModelForCTC

processor = AutoProcessor.from_pretrained("facebook/mms-1b-all")
model = AutoModelForCTC.from_pretrained("facebook/mms-1b-all")

def collate_fn(batch):
    batch = [item for item in batch if item is not None]
    if not batch:
        return None

    input_features = [item["input_features"] for item in batch]
    label_features = [item["labels"] for item in batch]
    texts = [item["text"] for item in batch]

    input_features = torch.nn.utils.rnn.pad_sequence(input_features, batch_first=True)

    padded_labels = processor.tokenizer.pad(
        {"input_ids": label_features},
        return_tensors="pt",
        padding="longest"
    )
    label_features = padded_labels["input_ids"]

    return {
        "input_features": input_features,
        "labels": label_features,
        "text": texts,
    }


train_dataset = AudioDataset(train_df, processor)
val_dataset = AudioDataset(val_df, processor)

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
optimizer = AdamW(model.parameters(), lr=1e-5)


for epoch in range(wandb.config.epochs):
    model.train()
    total_loss = 0

    for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1} Training"):
        if batch is None:
            print("Skipping empty batch")
            continue

        inputs = batch["input_features"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(input_values=inputs, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        total_loss += loss.item()
        wandb.log({"train/loss": loss.item()})

    avg_train_loss = total_loss / len(train_loader)
    val_wer, val_ter = evaluate(model, val_loader, processor, device)

    wandb.log({
        "epoch": epoch + 1,
        "train/loss_avg": avg_train_loss,
        "val/wer": val_wer,
        "val/ter": val_ter,
    })

    print(f"Epoch {epoch + 1} | Train Loss: {avg_train_loss:.4f} | Val WER: {val_wer:.4f} | Val TER: {val_ter:.4f}")